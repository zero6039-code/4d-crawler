import requests
from bs4 import BeautifulSoup
import json
import os
from datetime import datetime
import re

# ç›®æ ‡URL
URL = "https://4d4d.co/"

# å®šä¹‰éœ€è¦æå–çš„å…¬å¸åŠå…¶åœ¨é¡µé¢ä¸­å¯¹åº”çš„æ ‡è¯†
COMPANY_CONFIG = {
    "damacai": {"table_class": "resultdamacailable", "name": "Damacai 4D"},
    "magnum": {"table_class": "resultm4dlable", "name": "Magnum 4D"},
    "toto": {"table_class": "resulttotolable", "name": "Toto 4D"},
    "singapore": {"table_class": "resultsabahlable", "name": "Singapore 4D"},
    "damacai_1p3d": {"table_class": "resultdamacailable", "name": "Da Ma Cai 1+3D"},
    "sandakan": {"table_class": "resultstc4dlable", "name": "Sandakan 4D"},
    "sarawak_cashsweep": {"table_class": "resultsteclable", "name": "Cashweep 4D"},
    "sabah": {"table_class": "resultsabahlable", "name": "Sabah88 4D"},
    "sabah_lotto": {"table_class": "resultsabahlable", "name": "Sabah Lotto"},
    "sportstoto_fireball": {"table_class": "resulttotolable", "name": "SportsToto Fireball"},
    "grand_dragon": {"table_class": "resultdamacailable", "name": "Grand Dragon"},
    "singapore_toto": {"table_class": "resultsabahlable", "name": "Singapore Toto"},
    "sportstoto_lotto": {"table_class": "resulttotolable", "name": "SportsToto Lotto"},
    "magnum_jackpot_gold": {"table_class": "resultm4dlable", "name": "Magnum Jackpot Gold"},
    "sportstoto_5d": {"table_class": "resulttotolable", "name": "SportsToto 5D"},
    "sportstoto_6d": {"table_class": "resulttotolable", "name": "SportsToto 6D"},
    "magnum_life": {"table_class": "resultm4dlable", "name": "Magnum Life"},
}

def fetch_html():
    """è·å–é¡µé¢HTML"""
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36"
    }
    try:
        r = requests.get(URL, headers=headers, timeout=15)
        r.encoding = "utf-8"
        r.raise_for_status()
        return r.text
    except Exception as e:
        print(f"âŒ æŠ“å–å¤±è´¥: {e}")
        return None

def extract_company_data(soup, company_key):
    """æ ¹æ®å…¬å¸æ ‡è¯†ä»soupä¸­æå–æ•°æ®"""
    config = COMPANY_CONFIG[company_key]
    outer_boxes = soup.find_all("div", class_="outerbox")
    for box in outer_boxes:
        name_td = box.find("td", class_=config["table_class"])
        if name_td and config["name"] in name_td.get_text():
            return parse_outerbox(box, company_key)
    return None

def parse_outerbox(box, company_key):
    """è§£æå•ä¸ªouterboxå†…çš„æ•°æ®"""
    data = {
        "draw_date": "",
        "draw_no": "",
        "1st": "",
        "2nd": "",
        "3rd": "",
        "special": [],
        "consolation": [],
        "type": None
    }

    # æå–å¼€å¥–æ—¥æœŸå’ŒæœŸå·
    draw_row = box.find("td", class_="resultdrawdate")
    if draw_row:
        date_text = draw_row.get_text(strip=True)
        match = re.search(r"(\d{2}-\d{2}-\d{4})", date_text)
        if match:
            data["draw_date"] = match.group(1)
        next_td = draw_row.find_next("td", class_="resultdrawdate")
        if next_td:
            data["draw_no"] = next_td.get_text(strip=True).replace("Draw No:", "").strip()

    # æå–å‰ä¸‰å
    prize_tds = box.find_all("td", class_="resulttop")
    if len(prize_tds) >= 3:
        data["1st"] = prize_tds[0].get_text(strip=True)
        data["2nd"] = prize_tds[1].get_text(strip=True)
        data["3rd"] = prize_tds[2].get_text(strip=True)

    # ç‰¹åˆ«å¥–
    special_section = box.find("td", string=re.compile("Special|ç‰¹åˆ¥ç"))
    if special_section:
        table = special_section.find_parent("table")
        if table:
            rows = table.find_all("tr")
            special_numbers = []
            for row in rows[1:]:
                tds = row.find_all("td", class_="resultbottom")
                for td in tds:
                    num = td.get_text(strip=True)
                    if num and num != "----":
                        special_numbers.append(num)
            data["special"] = special_numbers

    # å®‰æ…°å¥–
    cons_section = box.find("td", string=re.compile("Consolation|å®‰æ…°ç"))
    if cons_section:
        table = cons_section.find_parent("table")
        if table:
            rows = table.find_all("tr")
            cons_numbers = []
            for row in rows[1:]:
                tds = row.find_all("td", class_="resultbottom")
                for td in tds:
                    num = td.get_text(strip=True)
                    if num and num != "----":
                        cons_numbers.append(num)
            data["consolation"] = cons_numbers

    # ç‰¹æ®Šå…¬å¸ç±»å‹æ ‡è®°
    if company_key in ["sportstoto_5d", "sportstoto_6d", "sportstoto_lotto", "singapore_toto", "magnum_jackpot_gold", "magnum_life"]:
        data["type"] = company_key

    return data

def save_json(company, data):
    """å°†æ•°æ®ä¿å­˜ä¸ºæœ€æ–°æ–‡ä»¶ï¼Œå¹¶å½’æ¡£åˆ°æ—¥æœŸå­ç›®å½•"""
    if not data:
        return

    base_dir = "docs/data"
    os.makedirs(base_dir, exist_ok=True)

    # 1. ä¿å­˜æœ€æ–°æ–‡ä»¶ï¼ˆè¦†ç›–ï¼‰
    latest_path = os.path.join(base_dir, f"{company}.json")
    with open(latest_path, "w", encoding="utf-8") as f:
        json.dump(data, f, ensure_ascii=False, indent=2)
    print(f"âœ… å·²æ›´æ–°æœ€æ–°æ–‡ä»¶: {latest_path}")

    # 2. è·å–å¼€å¥–æ—¥æœŸï¼ˆç”¨äºå½’æ¡£ç›®å½•ï¼‰
    draw_date = data.get("draw_date", "")
    if not draw_date or draw_date == "----":
        draw_date = datetime.now().strftime("%Y-%m-%d")
    else:
        try:
            # å‡è®¾ draw_date æ ¼å¼ä¸º DD-MM-YYYY
            d = datetime.strptime(draw_date, "%d-%m-%Y")
            draw_date = d.strftime("%Y-%m-%d")
        except:
            draw_date = datetime.now().strftime("%Y-%m-%d")

    # 3. å½’æ¡£åˆ°æ—¥æœŸç›®å½•
    archive_dir = os.path.join(base_dir, draw_date)
    os.makedirs(archive_dir, exist_ok=True)
    archive_path = os.path.join(archive_dir, f"{company}.json")
    with open(archive_path, "w", encoding="utf-8") as f:
        json.dump(data, f, ensure_ascii=False, indent=2)
    print(f"ğŸ“ å·²å½’æ¡£è‡³: {archive_path}")

def update_dates_index():
    """æ›´æ–° dates.json ç´¢å¼•æ–‡ä»¶ï¼Œåˆ—å‡ºæ‰€æœ‰å½’æ¡£æ—¥æœŸç›®å½•"""
    base_dir = "docs/data"
    if not os.path.exists(base_dir):
        return
    dates = []
    for item in os.listdir(base_dir):
        item_path = os.path.join(base_dir, item)
        if os.path.isdir(item_path) and re.match(r"\d{4}-\d{2}-\d{2}", item):
            dates.append(item)
    dates.sort(reverse=True)  # æœ€æ–°çš„é å‰
    index_path = os.path.join(base_dir, "dates.json")
    with open(index_path, "w", encoding="utf-8") as f:
        json.dump(dates, f)
    print(f"ğŸ“‹ å·²æ›´æ–°æ—¥æœŸç´¢å¼•ï¼Œå…± {len(dates)} ä¸ªå†å²æ—¥æœŸ")

def main():
    html = fetch_html()
    if not html:
        return
    soup = BeautifulSoup(html, "html.parser")

    # ä»ç¬¬ä¸€ä¸ªå…¬å¸è·å–å…¨å±€æ—¥æœŸï¼ˆå¯é€‰ï¼‰
    first_company = next(iter(COMPANY_CONFIG))
    first_data = extract_company_data(soup, first_company)
    if first_data:
        global_date = first_data["draw_date"]
        global_draw_no = first_data["draw_no"]
        with open("docs/data/latest.json", "w", encoding="utf-8") as f:
            json.dump({"draw_date": global_date, "draw_no": global_draw_no}, f)

    # é€ä¸ªæå–å¹¶ä¿å­˜
    for company in COMPANY_CONFIG:
        data = extract_company_data(soup, company)
        save_json(company, data)

    # æœ€åæ›´æ–°æ—¥æœŸç´¢å¼•
    update_dates_index()

if __name__ == "__main__":
    main()
