import requests
from bs4 import BeautifulSoup
import json
import os
from datetime import datetime
import re

URL = "https://4d4d.co/"

# å…¬å¸åç§°åˆ° key çš„æ˜ å°„ï¼ˆå¿…é¡»ä¸é¡µé¢æ˜¾ç¤ºå®Œå…¨ä¸€è‡´ï¼‰
COMPANY_NAME_TO_KEY = {
    "Damacai 4D": "damacai",
    "Magnum 4D": "magnum",
    "Toto 4D": "toto",
    "Singapore 4D": "singapore",
    "Da Ma Cai 1+3D": "damacai_1p3d",
    "Sandakan 4D": "sandakan",
    "Cashweep 4D": "sarawak_cashsweep",
    "Sabah88 4D": "sabah",
    "Sabah Lotto": "sabah_lotto",
    "SportsToto Fireball": "sportstoto_fireball",
    "Grand Dragon": "grand_dragon",
    "Singapore Toto": "singapore_toto",
    "SportsToto Lotto": "sportstoto_lotto",
    "Magnum Jackpot Gold": "magnum_jackpot_gold",
    "SportsToto 5D": "sportstoto_5d",
    "SportsToto 6D": "sportstoto_6d",
    "Magnum Life": "magnum_life",
}

# æ˜¾ç¤ºåç§°ï¼ˆå‰ç«¯ä½¿ç”¨ï¼‰
NAME_MAP = {
    "damacai": "DAMACAI 4D",
    "magnum": "MAGNUM 4D",
    "toto": "TOTO 4D",
    "singapore": "SINGAPORE 4D",
    "damacai_1p3d": "DAMACAI 1+3D",
    "sandakan": "SANDAKAN 4D",
    "sarawak_cashsweep": "SARAWAK CASHSWEEP",
    "sabah": "SABAH 88 4D",
    "sabah_lotto": "SABAH LOTTO",
    "sportstoto_fireball": "SPORTSTOTO FIREBALL",
    "grand_dragon": "GRAND DRAGON",
    "singapore_toto": "SINGAPORE TOTO",
    "sportstoto_lotto": "SPORTSTOTO LOTTO",
    "magnum_jackpot_gold": "MAGNUM JACKPOT GOLD",
    "sportstoto_5d": "SPORTSTOTO 5D",
    "sportstoto_6d": "SPORTSTOTO 6D",
    "magnum_life": "MAGNUM LIFE",
}

# éœ€è¦ç‰¹æ®Šæ¸²æŸ“çš„å…¬å¸åˆ—è¡¨
SPECIAL_COMPANIES = [
    "sportstoto_fireball", "sportstoto_lotto", "singapore_toto",
    "magnum_jackpot_gold", "sportstoto_5d", "sportstoto_6d", "magnum_life"
]

def fetch_html():
    headers = {"User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36"}
    try:
        r = requests.get(URL, headers=headers, timeout=15)
        r.encoding = "utf-8"
        r.raise_for_status()
        return r.text
    except Exception as e:
        print(f"âŒ æŠ“å–å¤±è´¥: {e}")
        return None

def extract_global_date(soup):
    """ä»é¡µé¢ç¬¬ä¸€ä¸ª outerbox æå–å…¨å±€æ—¥æœŸå’ŒæœŸå·"""
    first_box = soup.find("div", class_="outerbox")
    if not first_box:
        return None, None
    draw_row = first_box.find("td", class_="resultdrawdate")
    if not draw_row:
        return None, None
    date_text = draw_row.get_text(strip=True)
    match = re.search(r"(\d{2}-\d{2}-\d{4})", date_text)
    date = match.group(1) if match else None
    next_td = draw_row.find_next("td", class_="resultdrawdate")
    draw_no = None
    if next_td:
        no_text = next_td.get_text(strip=True)
        draw_no = re.sub(r"Draw No:?", "", no_text).strip()
    return date, draw_no

def parse_outerbox(box, global_date, global_draw_no):
    """è§£æå•ä¸ª outerboxï¼Œè¿”å› (company_key, data)"""
    # å°è¯•ä»å¤šç§æ–¹å¼è·å–å…¬å¸åç§°
    company_name = None

    # æ–¹æ³•1ï¼šæŸ¥æ‰¾å¯èƒ½åŒ…å«å…¬å¸åçš„ td
    possible_classes = ["resultdamacailable", "resultm4dlable", "resulttotolable", "resultsabahlable", "resultstc4dlable", "resultsteclable"]
    for cls in possible_classes:
        name_td = box.find("td", class_=cls)
        if name_td:
            text = name_td.get_text(strip=True)
            if text and not text.startswith(("img", "http")):
                company_name = text
                break

    # æ–¹æ³•2ï¼šä»å›¾ç‰‡ alt å±æ€§è·å–
    if not company_name:
        img = box.find("img")
        if img and img.get("alt"):
            company_name = img["alt"]

    if not company_name:
        print("âš ï¸ æ— æ³•è¯†åˆ«å…¬å¸åç§°")
        return None, None

    company_key = COMPANY_NAME_TO_KEY.get(company_name)
    if not company_key:
        print(f"âš ï¸ æœªçŸ¥å…¬å¸åç§°: {company_name}")
        return None, None

    data = {
        "draw_date": "",
        "draw_no": "",
        "1st": "",
        "2nd": "",
        "3rd": "",
        "special": [],
        "consolation": [],
        "type": company_key if company_key in SPECIAL_COMPANIES else None
    }

    # æå–è‡ªå·±çš„æ—¥æœŸå’ŒæœŸå·
    draw_row = box.find("td", class_="resultdrawdate")
    if draw_row:
        date_text = draw_row.get_text(strip=True)
        match = re.search(r"(\d{2}-\d{2}-\d{4})", date_text)
        if match:
            data["draw_date"] = match.group(1)
        next_td = draw_row.find_next("td", class_="resultdrawdate")
        if next_td:
            no_text = next_td.get_text(strip=True)
            data["draw_no"] = re.sub(r"Draw No:?", "", no_text).strip()

    # å¦‚æœæ²¡æœ‰è‡ªå·±çš„æ—¥æœŸï¼Œä½¿ç”¨å…¨å±€æ—¥æœŸ
    if not data["draw_date"] and global_date:
        data["draw_date"] = global_date
    if not data["draw_no"] and global_draw_no:
        data["draw_no"] = global_draw_no

    # å‰ä¸‰å
    prize_tds = box.find_all("td", class_="resulttop")
    if len(prize_tds) >= 3:
        data["1st"] = prize_tds[0].get_text(strip=True)
        data["2nd"] = prize_tds[1].get_text(strip=True)
        data["3rd"] = prize_tds[2].get_text(strip=True)
    else:
        print(f"âš ï¸ {company_key} æœªæ‰¾åˆ°å‰ä¸‰å")

    # ç‰¹åˆ«å¥–
    special_section = box.find("td", string=re.compile("Special|ç‰¹åˆ¥ç"))
    if special_section:
        table = special_section.find_parent("table")
        if table:
            rows = table.find_all("tr")
            special_numbers = []
            for row in rows[1:]:
                tds = row.find_all("td", class_="resultbottom")
                for td in tds:
                    num = td.get_text(strip=True)
                    if num and num != "----":
                        special_numbers.append(num)
            data["special"] = special_numbers

    # å®‰æ…°å¥–
    cons_section = box.find("td", string=re.compile("Consolation|å®‰æ…°ç"))
    if cons_section:
        table = cons_section.find_parent("table")
        if table:
            rows = table.find_all("tr")
            cons_numbers = []
            for row in rows[1:]:
                tds = row.find_all("td", class_="resultbottom")
                for td in tds:
                    num = td.get_text(strip=True)
                    if num and num != "----":
                        cons_numbers.append(num)
            data["consolation"] = cons_numbers

    return company_key, data

def save_json(company, data):
    if not data:
        print(f"âŒ {company} æ•°æ®ä¸ºç©ºï¼Œè·³è¿‡ä¿å­˜")
        return
    base_dir = "docs/data"
    os.makedirs(base_dir, exist_ok=True)

    # æœ€æ–°æ–‡ä»¶
    latest_path = os.path.join(base_dir, f"{company}.json")
    with open(latest_path, "w", encoding="utf-8") as f:
        json.dump(data, f, ensure_ascii=False, indent=2)
    print(f"âœ… å·²æ›´æ–°æœ€æ–°æ–‡ä»¶: {latest_path}")

    # å½’æ¡£
    draw_date = data.get("draw_date", "")
    if not draw_date or draw_date == "----":
        draw_date = datetime.now().strftime("%Y-%m-%d")
    else:
        try:
            d = datetime.strptime(draw_date, "%d-%m-%Y")
            draw_date = d.strftime("%Y-%m-%d")
        except:
            draw_date = datetime.now().strftime("%Y-%m-%d")

    archive_dir = os.path.join(base_dir, draw_date)
    os.makedirs(archive_dir, exist_ok=True)
    archive_path = os.path.join(archive_dir, f"{company}.json")
    with open(archive_path, "w", encoding="utf-8") as f:
        json.dump(data, f, ensure_ascii=False, indent=2)
    print(f"ğŸ“ å·²å½’æ¡£è‡³: {archive_path}")

def update_dates_index():
    base_dir = "docs/data"
    if not os.path.exists(base_dir):
        return
    dates = []
    for item in os.listdir(base_dir):
        item_path = os.path.join(base_dir, item)
        if os.path.isdir(item_path) and re.match(r"\d{4}-\d{2}-\d{2}", item):
            dates.append(item)
    dates.sort(reverse=True)
    index_path = os.path.join(base_dir, "dates.json")
    with open(index_path, "w", encoding="utf-8") as f:
        json.dump(dates, f)
    print(f"ğŸ“‹ å·²æ›´æ–°æ—¥æœŸç´¢å¼•ï¼Œå…± {len(dates)} ä¸ªå†å²æ—¥æœŸ")

def main():
    html = fetch_html()
    if not html:
        return
    soup = BeautifulSoup(html, "html.parser")

    global_date, global_draw_no = extract_global_date(soup)
    print(f"ğŸŒ å…¨å±€æ—¥æœŸ: {global_date}, å…¨å±€æœŸå·: {global_draw_no}")

    outer_boxes = soup.find_all("div", class_="outerbox")
    print(f"ğŸ“¦ æ‰¾åˆ° {len(outer_boxes)} ä¸ª outerbox")

    processed_keys = set()

    for idx, box in enumerate(outer_boxes):
        print(f"ğŸ” æ­£åœ¨è§£æç¬¬ {idx+1} ä¸ª outerbox...")
        company_key, data = parse_outerbox(box, global_date, global_draw_no)
        if company_key and data:
            if company_key in processed_keys:
                print(f"âš ï¸ é‡å¤çš„å…¬å¸ {company_key}ï¼Œè·³è¿‡")
                continue
            save_json(company_key, data)
            processed_keys.add(company_key)
        else:
            print(f"âš ï¸ ç¬¬ {idx+1} ä¸ª outerbox è§£æå¤±è´¥")

    # æ£€æŸ¥é—æ¼
    all_keys = set(COMPANY_NAME_TO_KEY.values())
    missing = all_keys - processed_keys
    if missing:
        print(f"âŒ ä»¥ä¸‹å…¬å¸æœªæ‰¾åˆ°: {missing}")
    else:
        print("âœ… æ‰€æœ‰å…¬å¸å‡å·²æˆåŠŸå¤„ç†")

    update_dates_index()

if __name__ == "__main__":
    main()
